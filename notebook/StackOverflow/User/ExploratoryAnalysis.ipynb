{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Python Logo](../../assets/images/python-logo-master-v3-TM.png)\n",
    "\n",
    "# <center> Exploratory Data Analysis for StackOverflow User Data </center>\n",
    "### <center> Shagun Sodhani </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# put in the name of the Stack Exchange Site which you want to analyse.\n",
    "data_source = \"StackOverflow\"\n",
    "data_type = \"User\"\n",
    "base_url = \"http://stackoverflow.com/users/\"\n",
    "local_user_data_path = \"../../data/\"+data_source+\"/\"+data_type\n",
    "use_local_datapath = True\n",
    "# If this is set to false, dbfs_data_path is used to read the data.\n",
    "dbfs_user_data_path = \"\"\n",
    "# dbfs_data_path is to be used when running the notebook on Databricks and data has been uploaded to dbfs. \n",
    "# For more details, refer https://github.com/shagunsodhani/PyCon2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Before we dive into the code, lets see the script which I used to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\" $SPARK_HOME/bin/pyspark --master local[2]\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../../run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**PYSPARK_DRIVER_PYTHON=jupyter**\n",
    "\n",
    "This sets the driver as this Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\"**\n",
    "\n",
    "This is used to pass options when starting the Python driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**$SPARK_HOME/bin/pyspark --master local[2]**\n",
    "\n",
    "This runs a pyspark shell in local mode with 2 executors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let us begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We will import everything in one cell for the sake for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import operator\n",
    "%matplotlib inline\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Notice that we did not import sparkcontext or sqlContext. They are already created for us to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Check if SparkContext and SQLContext have been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkContext is defined.\n"
     ]
    }
   ],
   "source": [
    "if('sc' in locals() or 'sc' in globals()):\n",
    "    print(\"SparkContext is defined.\")\n",
    "else:\n",
    "    print(\"SparkContext does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLContext is defined.\n"
     ]
    }
   ],
   "source": [
    "if('sqlContext' in locals() or 'sqlContext' in globals()):\n",
    "    print(\"SQLContext is defined.\")\n",
    "else:\n",
    "    print(\"SQLContext does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now let us load the user data as a dataframe. Since we will be using this data many times, we will cache it in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_source = StackOverflow\n",
      "data_type = User\n",
      "user_data_path = ../../data/StackOverflow/User\n",
      "Time taken = 24.7304959297 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "print (\"data_source =\", data_source)\n",
    "print (\"data_type =\", data_type)\n",
    "if(use_local_datapath):\n",
    "    user_data_path = local_user_data_path\n",
    "else:\n",
    "    user_data_path = dbfs_user_data_path\n",
    "print (\"user_data_path =\", user_data_path)\n",
    "user_df = sqlContext.read.load(user_data_path).cache()\n",
    "print (\"Time taken =\", time() - start_time, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### *sqlContext.read.load(user_data_path)* tells the master to load the data written in *user_data_path* as a parquet file (default choice for data format)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### *cache()* tells the master to cache the data in the main memory for faster operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> DataFrame </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Since DataFrame organizes the data into a relational view (rows and columns), it has a schema associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for user dataframe\n",
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Reputation: long (nullable = true)\n",
      " |-- CreationDate: string (nullable = true)\n",
      " |-- DisplayName: string (nullable = true)\n",
      " |-- EmailHash: string (nullable = true)\n",
      " |-- lastAccessDate: string (nullable = true)\n",
      " |-- WebsiteUrl: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- AboutMe: string (nullable = true)\n",
      " |-- Views: long (nullable = true)\n",
      " |-- UpVotes: long (nullable = true)\n",
      " |-- DownVotes: long (nullable = true)\n",
      " |-- AccountId: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Schema for user dataframe\")\n",
    "user_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some common metrics for this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in the data = 5677258\n"
     ]
    }
   ],
   "source": [
    "number_of_rows = user_df.count()\n",
    "print(\"Total number of rows in the data = \" + str(number_of_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique Id in the data = 5677258\n"
     ]
    }
   ],
   "source": [
    "number_of_unique_ids = user_df.select('Id').distinct().count()\n",
    "print(\"Total number of unique Id in the data = \" + str(number_of_unique_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum reputation for a user = 872346\n"
     ]
    }
   ],
   "source": [
    "max_reputation = user_df.agg({\"Reputation\": \"max\"}).collect()[0][0]\n",
    "print(\"Maximum reputation for a user = \" + str(max_reputation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum UpVotes for a user = 155541\n"
     ]
    }
   ],
   "source": [
    "max_upvotes = user_df.agg({\"UpVotes\": \"max\"}).collect()[0][0]\n",
    "print(\"Maximum UpVotes for a user = \" + str(max_upvotes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Alternatively, we could use the *describe* method to get all these statistics in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>Id</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>Age</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>DownVotes</th>\n",
       "      <th>AccountId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>5677258</td>\n",
       "      <td>5677258</td>\n",
       "      <td>5677258</td>\n",
       "      <td>5677258</td>\n",
       "      <td>5677258</td>\n",
       "      <td>5677258</td>\n",
       "      <td>5677258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>3458959.329377492</td>\n",
       "      <td>122.36497495798147</td>\n",
       "      <td>2.1128550085270037</td>\n",
       "      <td>14.985955015607887</td>\n",
       "      <td>13.056719106300964</td>\n",
       "      <td>1.5714251492533895</td>\n",
       "      <td>4239436.648318959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>1775717.819577991</td>\n",
       "      <td>2178.7325223376183</td>\n",
       "      <td>9.808645948617777</td>\n",
       "      <td>621.4858576805434</td>\n",
       "      <td>157.4016433378767</td>\n",
       "      <td>296.28945574718387</td>\n",
       "      <td>2463450.0689051063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-972</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>6455148</td>\n",
       "      <td>872346</td>\n",
       "      <td>96</td>\n",
       "      <td>1190669</td>\n",
       "      <td>155541</td>\n",
       "      <td>683400</td>\n",
       "      <td>8619800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                 Id          Reputation                 Age  \\\n",
       "0   count            5677258             5677258             5677258   \n",
       "1    mean  3458959.329377492  122.36497495798147  2.1128550085270037   \n",
       "2  stddev  1775717.819577991  2178.7325223376183   9.808645948617777   \n",
       "3     min                 -1                   1                -972   \n",
       "4     max            6455148              872346                  96   \n",
       "\n",
       "                Views             UpVotes           DownVotes  \\\n",
       "0             5677258             5677258             5677258   \n",
       "1  14.985955015607887  13.056719106300964  1.5714251492533895   \n",
       "2   621.4858576805434   157.4016433378767  296.28945574718387   \n",
       "3                   0                   0                   0   \n",
       "4             1190669              155541              683400   \n",
       "\n",
       "            AccountId  \n",
       "0             5677258  \n",
       "1   4239436.648318959  \n",
       "2  2463450.0689051063  \n",
       "3                  -2  \n",
       "4             8619800  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Disclaimer - Pandas is used only for visualising the processed data in tabular form as Jupyter notebooks visualise Pandas dataframes very nicely. All the processing is being done by Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let us find the user who has the highest reputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>EmailHash</th>\n",
       "      <th>lastAccessDate</th>\n",
       "      <th>WebsiteUrl</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "      <th>AboutMe</th>\n",
       "      <th>Views</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>DownVotes</th>\n",
       "      <th>AccountId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22656</td>\n",
       "      <td>872346</td>\n",
       "      <td>2008-09-26T12:05:05.150</td>\n",
       "      <td>Jon Skeet</td>\n",
       "      <td></td>\n",
       "      <td>2016-06-11T17:54:35.587</td>\n",
       "      <td>http://csharpindepth.com</td>\n",
       "      <td>Reading, United Kingdom</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;p&gt;\\nAuthor of &lt;a href=\"http://www.manning.com...</td>\n",
       "      <td>1190669</td>\n",
       "      <td>15469</td>\n",
       "      <td>5013</td>\n",
       "      <td>11683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  Reputation             CreationDate DisplayName EmailHash  \\\n",
       "0  22656      872346  2008-09-26T12:05:05.150   Jon Skeet             \n",
       "\n",
       "            lastAccessDate                WebsiteUrl                 Location  \\\n",
       "0  2016-06-11T17:54:35.587  http://csharpindepth.com  Reading, United Kingdom   \n",
       "\n",
       "   Age                                            AboutMe    Views  UpVotes  \\\n",
       "0   40  <p>\\nAuthor of <a href=\"http://www.manning.com...  1190669    15469   \n",
       "\n",
       "   DownVotes  AccountId  \n",
       "0       5013      11683  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_max_reputation = user_df.filter(\"Reputation = \"+str(max_reputation)).select('Id').collect()[0][0]\n",
    "user_df.filter(\"Reputation = \"+str(max_reputation)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  <center> Lazy Evaluation </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Spark Dataframes and RDDs are lazy. They do nothing unless an action is called. We can keep adding as many transformations as we want but nothing actually happens till we call an action. By deferring evaluations, Spark can optimise the overall workflow as it evaluates only that which is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Id: bigint, Reputation: bigint, CreationDate: string, DisplayName: string, EmailHash: string, lastAccessDate: string, WebsiteUrl: string, Location: string, Age: bigint, AboutMe: string, Views: bigint, UpVotes: bigint, DownVotes: bigint, AccountId: bigint]\n",
      "Time taken = 0.0157780647278 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "user_max_reputation = user_df.orderBy(\"Reputation\", desc=True)\n",
    "print (user_max_reputation)\n",
    "print (\"Time taken =\", time() - start_time, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This command crashes.\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "# list_user_max_reputation = user_df.orderBy(\"Reputation\", desc=True).collect()\n",
    "print (\"This command crashes.\")\n",
    "# print (\"Time taken =\", time() - start_time, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of list = 1\n",
      "Time taken = 30.4480409622 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "list_user_max_reputation = user_df.orderBy(\"Reputation\", desc=True).take(1)\n",
    "print (\"Size of list =\",len(list_user_max_reputation))\n",
    "print (\"Time taken =\", time() - start_time, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lazy is better than eager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We can find the user with maximum number of upvotes, downvotes and views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max Reputation value = 872346 for userId = 22656  http://stackoverflow.com/users/22656 \n",
      "\n",
      "max UpVotes value = 155541 for userId = -1  http://stackoverflow.com/users/-1 \n",
      "\n",
      "max DownVotes value = 683400 for userId = -1  http://stackoverflow.com/users/-1 \n",
      "\n",
      "max Views value = 1190669 for userId = 22656  http://stackoverflow.com/users/22656 \n",
      "\n",
      "max Age value = 96 for userId = 4379582  http://stackoverflow.com/users/4379582 \n",
      "\n",
      "Time taken = 37.6267881393 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time() \n",
    "column_list = [\"Reputation\", \"UpVotes\", \"DownVotes\", \"Views\", \"Age\"]\n",
    "\n",
    "for col in column_list:\n",
    "    max_value_col = user_df.orderBy(col, ascending=False).select(\"Id\", col).take(1)[0]\n",
    "    userId = str(max_value_col[0])\n",
    "    print (\"max\", col, \"value =\",str(max_value_col[1]),\"for userId =\", userId, \"\", base_url+userId, \"\\n\")\n",
    "print (\"Time taken =\", time() - start_time, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Interestingly, the user with most upvotes is also the most viewed user. The userId = -1 corresponds to community (bot) user which helps in moderation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Before using any algorithm from Spark Machine Learning (ml and MLlib), we transform this data to a collection of vectors. While parsing the raw xml files, I used certain default values (in place of missing values). We will filter for those values now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "filter_expression = \"AccountId > -1 and DownVotes > -1 and UpVotes > -1 and \\\n",
    "Views > -1 and Reputation > -9223372036854775808 and Age > 0\"\n",
    "vectorData = user_df.filter(filter_expression)\\\n",
    ".select(column_list)\\\n",
    ".map(lambda data: Vectors.dense([c for c in data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### *vectorData* consists of rows, where each row is a vector of data we selected above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[118] at RDD at PythonRDD.scala:43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[DenseVector([11.0, 0.0, 0.0, 5.0, 29.0]),\n",
       " DenseVector([21.0, 1.0, 0.0, 9.0, 34.0]),\n",
       " DenseVector([1.0, 0.0, 0.0, 0.0, 27.0]),\n",
       " DenseVector([336.0, 7.0, 1.0, 67.0, 20.0]),\n",
       " DenseVector([119.0, 17.0, 0.0, 23.0, 25.0])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (vectorData)\n",
    "vectorData.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Calculating basic statistics about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in vectorData are ['Reputation', 'UpVotes', 'DownVotes', 'Views', 'Age']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-20734b50fb08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Columns in vectorData are\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumn_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStatistics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolStats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Average \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shagun/devsetup/spark-1.6.1-bin-hadoop2.6/python/pyspark/mllib/stat/_statistics.pyc\u001b[0m in \u001b[0;36mcolStats\u001b[1;34m(rdd)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;36m2.\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mcStats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"colStats\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_to_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mMultivariateStatisticalSummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcStats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shagun/devsetup/spark-1.6.1-bin-hadoop2.6/python/pyspark/mllib/common.pyc\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shagun/devsetup/spark-1.6.1-bin-hadoop2.6/python/pyspark/mllib/common.pyc\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shagun/devsetup/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m    813\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32m/home/shagun/devsetup/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry)\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_give_back_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shagun/devsetup/spark-1.6.1-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m             \u001b[1;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    428\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m                         \u001b[1;32mwhile\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m                             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m                                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print (\"Columns in vectorData are\",column_list)\n",
    "stats = Statistics.colStats(vectorData)\n",
    "for col, val in zip(column_list, stats.mean()):\n",
    "    print (\"Average \", col, \" = \", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Other supported statistics include:\n",
    "    * count()\n",
    "    * max()\n",
    "    * mean()\n",
    "    * min()\n",
    "    * normL1()\n",
    "    * normL2()\n",
    "    * numNonzeros()\n",
    "    * variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the sake of completeness, we will try all these methods. We would visualise this data as Pandas dataframe to make it look clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd_data = map(list, [[stats.count()]*5, stats.max(), stats.mean(), stats.min(), stats.normL1(), stats.normL2(),\\\n",
    "           stats.numNonzeros() ,stats.variance()])\n",
    "pd_index = ['Count', 'Max', 'Mean', 'Min', 'NormL1', 'NormL2', 'NumNonzeros', 'Variance']\n",
    "pd.DataFrame(pd_data, index=pd_index, columns=column_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We will calculate correlation between each pair of columns and see if we find some thing interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "Statistics.colStats(vectorData)\n",
    "pd_data = Statistics.corr(vectorData, method=\"pearson\")\n",
    "pd.DataFrame(pd_data, index=column_list, columns=column_list)\n",
    "print (\"Time taken =\", time() - start_time, \"seconds.\")\n",
    "pd.DataFrame(pd_data, index=column_list, columns=column_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Some observations:\n",
    "    High positive correlation between Reputation and Upvotes (0.57).\n",
    "    \n",
    "    High positive correlation between Reputation and Views (0.56)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The last observation can be explained by the hypothesis that more reputation you have, more likely it is that people would land up on your profile page. Lets drill down further on the first observation.\n",
    "\n",
    "We filter the data to contain only Reputation and UpVotes and filter for rows with defualt values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "columns_to_select = [\"Reputation\", \"UpVotes\"]\n",
    "data = user_df.filter(filter_expression).select(columns_to_select).cache()\n",
    "data.sample(False, 0.0001, 42).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice that this is the second dataframe that we have cached so far.\n",
    "\n",
    "We can check the memory profile for the app at http://localhost:4040/storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We no longer need the *user_df* dataframe so we will unpersist it and recover some memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "user_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Check that now only one dataframe is cached for the application. http://localhost:4040/storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Spark is good at number chrunching and performing compuations but not so good at visualisations. So we will load a sample of the dataset into Pandas dataframe using the toPandas method and visualise it using sns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pandas_df = data.sample(False, .1, 42).toPandas()\n",
    "# %matplotlib qt\n",
    "sns.jointplot('Reputation', 'UpVotes', data=pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "pandas_df = data.sample(False, .1, 42).toPandas()\n",
    "%matplotlib inline\n",
    "sns.jointplot('Reputation', 'UpVotes', data=pandas_df)\n",
    "print (\"Time taken =\", time() - start_time, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "pandas_df = data.sample(False, 0.9, 42).toPandas()\n",
    "%matplotlib inline\n",
    "sns.jointplot('Reputation', 'UpVotes', data=pandas_df)\n",
    "print (\"Time taken =\", time() - start_time, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This visualistion is unintutive as most of the data is distributed in a narrow zone and the \n",
    "scale is skewed due to outliers. We will try plotting the values on the log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "log_pandas_df = data.select(func.log10(data[\"Reputation\"]),func.log10(data[\"UpVotes\"])).sample(False, .1).toPandas()\n",
    "# %matplotlib qt\n",
    "sns.jointplot('LOG10(Reputation)', 'LOG10(UpVotes)', data=log_pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "log_pandas_df = data.select(func.log10(data[\"Reputation\"]),func.log10(data[\"UpVotes\"])).sample(False, .1).toPandas()\n",
    "%matplotlib inline\n",
    "sns.jointplot('LOG10(Reputation)', 'LOG10(UpVotes)', data=log_pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before we wrap up, let us play with the *CreationDate* column and see if we can mine some interesting observations on that front. The idea is to show how we can play with more complex data types (like datetime)and use some Spark operators as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will start with the *user_df* dataframe. Remember that we had uncached it which means it is no longer available in memory but we can still run operations on it. In this case, the dataframe would be loaded once again but will not be persisted, unless we explicitly ask Spark to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "creation_date_df = user_df.select(\"CreationDate\")\n",
    "creation_date_df.printSchema()\n",
    "creation_date_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that **CreationDate** column was parsed as **string**. Let us fix that by selecting **CreationDate** as **date** type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "date_df = creation_date_df.select(func.to_date(creation_date_df[\"CreationDate\"]).alias(\"CreationDate\"))\n",
    "date_df.printSchema()\n",
    "date_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RDD vs DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want to count the number of new accounts each day. We would try doing this using both RDDs and DataFrames and compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RDD\n",
    "Convert dataframe to rdd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Map each row with numerical value 1 (number of accounts each row contributes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then we reduce our data using date as the key and add the values corresponding to same date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "date_rdd = date_df.rdd\n",
    "aggregated_df = date_rdd.map(lambda a: (a, 1)).groupByKey().map(lambda a: (a[0], sum(a[1])))\n",
    "print (\"aggregated_df maps each date to number of accounts that were created.\")\n",
    "print (\"Number of rows = \",aggregated_df.count())\n",
    "time_taken_rdd = time() - start_time\n",
    "print (\"Time taken =\", time_taken_rdd, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "aggregated_df = date_df.groupBy('CreationDate').count()\n",
    "print (\"aggregated_df maps each date to number of accounts that were created.\")\n",
    "print (\"Number of rows = \",aggregated_df.count())\n",
    "time_taken_df = time() - start_time\n",
    "print (\"Time taken =\", time_taken_df, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print (\"time_taken_rdd =\", time_taken_rdd)\n",
    "print (\"time_taken_df =\", time_taken_df)\n",
    "print (\"Speed up =\",time_taken_rdd/time_taken_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### DataFrames are many times faster than RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "aggregated_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pandas_df = aggregated_df.select(\"CreationDate\", aggregated_df[\"count\"].alias(\"NewAccounts\"))\\\n",
    ".orderBy(func.asc(\"CreationDate\")).toPandas()\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us plot this data and see if there is any interesting observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "plt.plot(pandas_df.CreationDate, pandas_df.NewAccounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(pandas_df.CreationDate, pandas_df.NewAccounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Interesting! We can see a spike in the initial data. If we zoom in, we see the date of the spike as 15th September,2008 when over 2000 new users joined StackOverflow. Flipping the pages of history, this was the day when StackOverflow was opened for public."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
